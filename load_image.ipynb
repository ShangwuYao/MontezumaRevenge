{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "image = np.load(\"image.npy\") \n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(image, 'RGB')  # image should be (h, w, 3)\n",
    "img.save('monte_stair.png')\n",
    "#img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import namedtuple\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\n",
    "from inferno.trainers.basic import Trainer\n",
    "from inferno.trainers.callbacks.base import Callback\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "def tuple_tostring(tuple):\n",
    "\treturn ','.join(tuple[1:-2].split(\" \"))\n",
    "\n",
    "def parse_txt(full_textname): \n",
    "\tfile_ = open(full_textname)\n",
    "\tparsed_text = list(filter(None, [tuple_tostring(line) for line in file_]))\n",
    "\tfile_.close()\t\n",
    "\treturn parsed_text\n",
    "\n",
    "def parse_annotation(text_dir, img_dir, label_file):\n",
    "    '''\n",
    "    new parse_annotation code\n",
    "    example use:\n",
    "    text_dir = '/Users/sw/programming/10703/project/yolo-boundingbox-labeler-GUI/bbox_txt'\n",
    "    img_dir = '/Users/sw/programming/10703/project/yolo-boundingbox-labeler-GUI/images'\n",
    "    '''\n",
    "    # LABELS dict\n",
    "    labels_ = parse_txt(label_file)\n",
    "    LABELS = {}\n",
    "    IDX_TO_LABELSTR = {}\n",
    "    i = 0\n",
    "    for label in labels_:\n",
    "        LABELS[label] = i\n",
    "        IDX_TO_LABELSTR[i] = label\n",
    "        i += 1\n",
    "    CLASS = len(LABELS)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_labels = []\n",
    "\n",
    "    file_names = [name for name in os.listdir(text_dir) if name.endswith('.txt')] # based on those with bboxes\n",
    "\n",
    "    for file_name in file_names:\n",
    "        img = {'object':[]}\n",
    "\n",
    "        full_textname = text_dir + '/' + file_name\n",
    "        full_imgname = img_dir + '/' + file_name.replace('txt', 'png') # TODO\n",
    "\n",
    "        img_label = parse_txt(full_textname)\n",
    "\n",
    "        label_encode = np.zeros((1,CLASS)).astype(int)\n",
    "        for label in img_label:\n",
    "            label_encode[0, LABELS[label]] = 1\n",
    "\n",
    "        im = Image.open(full_imgname)\n",
    "        image = np.array(im)\n",
    "        im.close()\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #img_ = Image.fromarray(image, 'L')\n",
    "        #img_.show()\n",
    "        \n",
    "        image = cv2.resize(image, (84, 84), interpolation = cv2.INTER_CUBIC)[np.newaxis,np.newaxis,:,:]\n",
    "        \n",
    "        all_labels.append(label_encode)\n",
    "        all_imgs.append(image)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\t\n",
    "    all_imgs = np.concatenate(all_imgs)\t\n",
    "\n",
    "    return all_imgs, all_labels, LABELS, IDX_TO_LABELSTR, CLASS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "    def __init__(self, args, dataloader, valid_dataloader, CLASSES, IDX_TO_LABELSTR, dropout=0.5):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.IDX_TO_LABELSTR = IDX_TO_LABELSTR\n",
    "        self.CLASSES = CLASSES\n",
    "        print(CLASSES)\n",
    "\n",
    "        # input (1, 84, 84)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #self.bn1 = nn.BatchNorm1d(256)\n",
    "        #self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #self.bn2 = nn.BatchNorm1d(384)\n",
    "        #self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #self.bn3 = nn.BatchNorm1d(512)\n",
    "        #self.drop3 = nn.Dropout(dropout)\n",
    "\n",
    "        # flatten\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features=3136,\n",
    "                                    out_features=256)\n",
    "        self.relulinear1 = nn.ReLU()\n",
    "        #self.droplinear1 = nn.Dropout(0.15)\n",
    "\n",
    "        for i, class_num in enumerate(CLASSES):\n",
    "            setattr(self, 'projection_{}'.format(i), nn.Linear(in_features=256,\n",
    "                                        out_features=class_num))\n",
    "            #self.projection = nn.Linear(in_features=256,\n",
    "            #                            out_features=CLASSES[0])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # initialization\n",
    "        #self.apply(wsj_initializer)\n",
    "\n",
    "        self.args = args\n",
    "        self.dataloader = dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.best_validation_acc = 0\n",
    "        self.model_param_str = 'weights'\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=args.init_lr)\n",
    "        #self.optimizer = optim.RMSprop(self.parameters(), lr=args.init_lr)\n",
    "        #self.optimizer = optim.SGD(self.parameters(), lr=args.init_lr)\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu1(self.conv1(x))\n",
    "        h = self.relu2(self.conv2(h))\n",
    "        h = self.relu3(self.conv3(h))\n",
    "\n",
    "        # flatten\n",
    "        h_size = h.size()\n",
    "        h = h.view(h_size[0], -1)\n",
    "\n",
    "        h = self.relulinear1(self.linear1(h))\n",
    "\n",
    "        #output = self.sigmoid(self.projection(h))\n",
    "        outputs = []\n",
    "        for i in range(len(CLASSES)):\n",
    "            outputs.append(self.sigmoid(getattr(self, 'projection_{}'.format(i))(h)))\n",
    "            \n",
    "        outputs = torch.cat(outputs, 1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def model_train(self):\n",
    "        for i in range(self.args.epochs):\n",
    "            print(\"---------epoch {}---------\".format(i))\n",
    "            start_time = time.time()\n",
    "            self.train()  # right place\n",
    "\n",
    "            losses = 0\n",
    "            total_cnt = 0\n",
    "\n",
    "            for input_x, label in self.dataloader:\n",
    "                self.zero_grad()\n",
    "\n",
    "                output = self.forward(to_variable(input_x)) \n",
    "\n",
    "                loss = self.criterion(output, to_variable(label))\n",
    "\n",
    "                total_cnt += 1\n",
    "                losses += loss.data[0]\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            print(\"training loss: {}\".format(losses / total_cnt / self.args.batch_size))\n",
    "            validation_acc = self.evaluate()\n",
    "            if validation_acc > self.best_validation_acc:\n",
    "                print(\"--------saving best model--------\")\n",
    "                self.model_param_str = \\\n",
    "                    'parser_epoch_{}_loss_{}_valacc_{}'.format(\n",
    "                        i, losses / total_cnt / self.args.batch_size, validation_acc)\n",
    "                torch.save(self.state_dict(), self.model_param_str + '.t7')\n",
    "                self.best_validation_acc = validation_acc\n",
    "\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\t\n",
    "\n",
    "        return self.model_param_str\t\t\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.eval()\n",
    "\n",
    "        losses = 0\n",
    "        total_cnt = 0\n",
    "        validation_accs = []\n",
    "        for input_x, label in self.valid_dataloader:\n",
    "            total_cnt += 1\n",
    "            output = self.forward(to_variable(input_x))\n",
    "\n",
    "            loss = self.criterion(output, to_variable(label))\n",
    "            losses += loss.data[0]\n",
    "            \n",
    "            output = output.data\n",
    "            cond1 = output < 0.5\n",
    "            cond2 = output >= 0.5\n",
    "            output[cond1] = 0\n",
    "            output[cond2] = 1\n",
    "            shape = output.shape\n",
    "            #print(\"shape:\", shape) # (B, Class_num)\n",
    "            #print(\"output\", output)\n",
    "            #print(\"label\", label)\n",
    "            validation_accs.append(torch.sum(output == label) / shape[0] / shape[1])\n",
    "\n",
    "        losses /= total_cnt * self.args.batch_size\n",
    "        print(\"validation loss: {}\".format(losses))\n",
    "        validation_acc = np.mean(validation_accs)\n",
    "        print(\"validation accuracy: {}\".format(validation_acc))\n",
    "        return validation_acc\n",
    "    \n",
    "    def decode_state(self, input_x):\n",
    "        self.eval()\n",
    "        \n",
    "        output = self.forward(to_variable(input_x))\n",
    "\n",
    "        output = output.data.numpy()[0] # TODO\n",
    "        decoded_state = get_state(output, self.IDX_TO_LABELSTR)\n",
    "        \n",
    "        return decoded_state\n",
    "\n",
    "def get_state(img_label, IDX_TO_LABELSTR):\n",
    "    labels = []\n",
    "    for i, logit in enumerate(img_label):\n",
    "        if logit >= 0.5:\n",
    "            labels.append(IDX_TO_LABELSTR[i])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(numpy_array, datatype):\n",
    "\t# Numpy array -> Tensor\n",
    "\tif datatype == 'int':\n",
    "\t\treturn torch.from_numpy(numpy_array).int()\n",
    "\telif datatype == 'long':\n",
    "\t\treturn torch.from_numpy(numpy_array).long()\n",
    "\telse:\n",
    "\t\treturn torch.from_numpy(numpy_array).float()\n",
    "\n",
    "\n",
    "def to_variable(tensor, cpu=False):\n",
    "\t# Tensor -> Variable (on GPU if possible)\n",
    "\tif torch.cuda.is_available() and not cpu:\n",
    "\t\t# Tensor -> GPU Tensor\n",
    "\t\ttensor = tensor.cuda()\n",
    "\treturn torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, input_x, labels, test=False):\n",
    "\t\tself.input_x = torch.from_numpy(input_x).float()\n",
    "\t\tself.labels = torch.from_numpy(labels).float()\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.input_x[index], self.labels[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8\n",
    "\n",
    "all_imgs, all_labels, LABELS, IDX_TO_LABELSTR, CLASS = parse_annotation('lev1_labeled/imgLevel1Label', 'lev1_labeled/imgLevel1Label', 'lev1_labeled/0_allpossible.txt')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        all_imgs, all_labels, random_state=6060, train_size=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = namedtuple('args',\n",
    "                  [\n",
    "                      'batch_size',\n",
    "                      'save_directory',\n",
    "                      'epochs',\n",
    "                      'init_lr',\n",
    "                      'cuda'])(\n",
    "    32,\n",
    "    'save_weights/',\n",
    "    40,\n",
    "    1e-4,\n",
    "    False)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = DataLoader(\n",
    "    myDataset(X_train, y_train), shuffle=True,\n",
    "    batch_size=args.batch_size, **kwargs)\n",
    "valid_loader = DataLoader(\n",
    "    myDataset(X_valid, y_valid), shuffle=True,\n",
    "    batch_size=args.batch_size, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 3]\n"
     ]
    }
   ],
   "source": [
    "CLASSES = [15, 3]\n",
    "model = CNNModel(args, train_loader, valid_loader, CLASSES, IDX_TO_LABELSTR, dropout=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_str = 'parser_epoch_23_loss_0.0002506485904053658_valacc_0.99765625'\n",
    "pretrained_dict = torch.load(model_param_str + '.t7')\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'unexpected key \"projection.weight\" in state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-adef4fbbed94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_param_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'parser_epoch_23_loss_0.0002506485904053658_valacc_0.99765625'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_param_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".t7\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 raise KeyError('unexpected key \"{}\" in state_dict'\n\u001b[0;32m--> 490\u001b[0;31m                                .format(name))\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'unexpected key \"projection.weight\" in state_dict'"
     ]
    }
   ],
   "source": [
    "model_param_str = 'parser_epoch_23_loss_0.0002506485904053658_valacc_0.99765625'\n",
    "model.load_state_dict(torch.load(model_param_str + \".t7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "[torch.FloatTensor of size 1x1x84x84]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      " 1.0000e+00  3.0032e-08  1.6336e-02  6.1793e-03  7.0580e-04  2.4750e-05\n",
      "\n",
      "Columns 6 to 11 \n",
      " 9.9755e-01  2.1443e-03  3.0528e-04  2.5332e-06  1.0000e+00  8.4269e-06\n",
      "\n",
      "Columns 12 to 14 \n",
      " 9.9999e-01  1.0000e+00  9.9827e-01\n",
      "[torch.FloatTensor of size 1x15]\n",
      "\n",
      "[9.9999976e-01 3.0031810e-08 1.6336488e-02 6.1792973e-03 7.0579525e-04\n",
      " 2.4749941e-05 9.9754673e-01 2.1443346e-03 3.0527558e-04 2.5331933e-06\n",
      " 9.9999750e-01 8.4269141e-06 9.9998593e-01 9.9999976e-01 9.9826652e-01]\n",
      "['actorInRoom,room_1', 'actorOnSpot,room_1,conveyor_1', 'keyExists,room_1,key_1', 'doorExists,room_1,door_1', 'doorExists,room_1,door_2', 'monsterExists,room_1,skull_1']\n"
     ]
    }
   ],
   "source": [
    "for x in valid_loader:\n",
    "    #print(x)\n",
    "    model.decode_state(x[0][1:2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actorInRoom,room_1': 0,\n",
       " 'actorInRoom,room_2': 1,\n",
       " 'actorOnSpot,room_1,chain_1': 5,\n",
       " 'actorOnSpot,room_1,conveyor_1': 6,\n",
       " 'actorOnSpot,room_1,conveyor_2': 7,\n",
       " 'actorOnSpot,room_1,entrance_1': 8,\n",
       " 'actorOnSpot,room_1,entrance_2': 9,\n",
       " 'actorOnSpot,room_1,ladder_1': 2,\n",
       " 'actorOnSpot,room_1,ladder_2': 3,\n",
       " 'actorOnSpot,room_1,ladder_3': 4,\n",
       " 'actorWithKey': 11,\n",
       " 'doorExists,room_1,door_1': 12,\n",
       " 'doorExists,room_1,door_2': 13,\n",
       " 'keyExists,room_1,key_1': 10,\n",
       " 'monsterExists,room_1,skull_1': 14}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.Tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1])\n",
    "model.decode_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.02394154667854309\n",
      "validation accuracy: 0.49291666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49291666666666667"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch 0---------\n",
      "training loss: 0.00879058318043297\n",
      "validation loss: 0.006028550677001476\n",
      "validation accuracy: 0.9573437499999999\n",
      "--------saving best model--------\n",
      "--- 1.3628971576690674 seconds ---\n",
      "---------epoch 1---------\n",
      "training loss: 0.004899389925412834\n",
      "validation loss: 0.004130142246140167\n",
      "validation accuracy: 0.9674479166666666\n",
      "--------saving best model--------\n",
      "--- 1.3731269836425781 seconds ---\n",
      "---------epoch 2---------\n",
      "training loss: 0.0033585435012355447\n",
      "validation loss: 0.0028550749411806464\n",
      "validation accuracy: 0.9769791666666667\n",
      "--------saving best model--------\n",
      "--- 1.3687889575958252 seconds ---\n",
      "---------epoch 3---------\n",
      "training loss: 0.0025308078408918596\n",
      "validation loss: 0.002140058742952533\n",
      "validation accuracy: 0.9864583333333333\n",
      "--------saving best model--------\n",
      "--- 1.3870577812194824 seconds ---\n",
      "---------epoch 4---------\n",
      "training loss: 0.0019815760632892225\n",
      "validation loss: 0.0019965337560279295\n",
      "validation accuracy: 0.9819270833333333\n",
      "--- 1.3788917064666748 seconds ---\n",
      "---------epoch 5---------\n",
      "training loss: 0.0016142583335749805\n",
      "validation loss: 0.0014362721121869981\n",
      "validation accuracy: 0.9893229166666666\n",
      "--------saving best model--------\n",
      "--- 1.3574011325836182 seconds ---\n",
      "---------epoch 6---------\n",
      "training loss: 0.0012941979512106627\n",
      "validation loss: 0.0012625391900655814\n",
      "validation accuracy: 0.9853645833333333\n",
      "--- 1.371908187866211 seconds ---\n",
      "---------epoch 7---------\n",
      "training loss: 0.0011168392004699192\n",
      "validation loss: 0.0007887258325354196\n",
      "validation accuracy: 0.9931770833333333\n",
      "--------saving best model--------\n",
      "--- 1.4049012660980225 seconds ---\n",
      "---------epoch 8---------\n",
      "training loss: 0.0009248957582961091\n",
      "validation loss: 0.0006630523130297661\n",
      "validation accuracy: 0.9944791666666668\n",
      "--------saving best model--------\n",
      "--- 1.378270149230957 seconds ---\n",
      "---------epoch 9---------\n",
      "training loss: 0.0007152738822200758\n",
      "validation loss: 0.0007020527009444777\n",
      "validation accuracy: 0.9931770833333333\n",
      "--- 1.3601160049438477 seconds ---\n",
      "---------epoch 10---------\n",
      "training loss: 0.000730990889926695\n",
      "validation loss: 0.0006111525835876819\n",
      "validation accuracy: 0.9973958333333333\n",
      "--------saving best model--------\n",
      "--- 1.3851518630981445 seconds ---\n",
      "---------epoch 11---------\n",
      "training loss: 0.0006315997677368366\n",
      "validation loss: 0.0004493755113799125\n",
      "validation accuracy: 0.9955208333333334\n",
      "--- 1.3643479347229004 seconds ---\n",
      "---------epoch 12---------\n",
      "training loss: 0.0005356722381706772\n",
      "validation loss: 0.0004186204623692902\n",
      "validation accuracy: 0.9970833333333333\n",
      "--- 1.3136019706726074 seconds ---\n",
      "---------epoch 13---------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-be8d164c669f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-183-4d853d171eaa>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-4d853d171eaa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.model_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d (1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d (32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (linear1): Linear(in_features=3136, out_features=256)\n",
       "  (relulinear1): ReLU()\n",
       "  (projection): Linear(in_features=256, out_features=15)\n",
       "  (criterion): BCELoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * no arguments\n * (torch.ByteTensor other, *, torch.ByteTensor out)\n      didn't match because some of the keywords were incorrect: axis\n * (int dim, *, tuple[torch.ByteTensor, torch.LongTensor] out)\n * (int dim, bool keepdim, *, tuple[torch.ByteTensor, torch.LongTensor] out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a3d50840129a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n",
      "\u001b[0;31mTypeError\u001b[0m: max received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * no arguments\n * (torch.ByteTensor other, *, torch.ByteTensor out)\n      didn't match because some of the keywords were incorrect: axis\n * (int dim, *, tuple[torch.ByteTensor, torch.LongTensor] out)\n * (int dim, bool keepdim, *, tuple[torch.ByteTensor, torch.LongTensor] out)\n"
     ]
    }
   ],
   "source": [
    "for x in valid_loader:\n",
    "    print(np.max(x[0]))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
